[TOC]

> 这两本书的标记符号是通用的。

文章总目录为[概率机器学习自学笔记目录 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/699377272?)。

## 1.1 常见数学符号

$\infty$    : 无穷大(infinity)

$\to$    : 趋近于(tends toward)

$\propto$     : 成正比(proportional to)

$\triangleq$     : 定义(defined as)

$O(\cdot)$   : 大O表示算法复杂度

$\mathbb Z_+$   : 正整数(positive integers)

$\mathbb R$      : 实数(real numbers)                                 

$\mathbb R_+$   : 正实数(positive reals)   

$\mathcal S_K$   : K维的概率单纯形(K-dimensional probability simplex) 

$\mathcal S_{++}^D$ : D阶正定矩阵(positive difinite matirces)

$\approx$     : 约等于(approximately equal to)

$\{1,...,N\}$  : 有限集(finite set)$\{1,...,N\}$

$1:N$  : 有限集$\{1,...,N\}$

$[\ell,u]$   : 连续区间(continuous interval)$\ell \leq x \leq u$

## 1.2 函数

我们通常记一个函数为$f$或者$g, h$. 

如果一个函数有一个固定的参数(fixed parameters)$\theta$, 我们可以记作$f(x;\theta)$或者$f_\theta(x)$. 

#### 1.2.1 常见的一元函数

$\lfloor x \rfloor$ : 向下取整(floor of x)                                      

$\lceil x \rceil$ : 向上取整(ceil of x)                                          

$\neg a$   : 逻辑非(logical NOT)                                          

$\mathbb I(x)$ : 示性函数(indicator function)                                 

$\delta (x)$ : 狄拉克$\delta $函数(Dirac delta), $x=0$时, $\delta(x)=\infty$, 否则$\delta(x)=0$. 

$|x|$    : 绝对值(absolute value)                                       

$|\mathcal S|$    : 集合的大小/基数(size/cardinality of a set)

$n!$      : 阶乘函数(factorial function)

$\log(x)$  : x的自然对数(natural logarithm)

$\exp(x)$ : 指数函数$e^x$(exponential function)

$\Gamma (x)$     : 伽马函数(Gamma function)$\Gamma(x)=\int_0^\infty u^{x-1}e^{-u}du$

$\Psi (x)$    : 双伽马函数(Digamma function)$\Psi(x)=\frac{d}{dx}\operatorname{log}\Gamma(x)$

$\sigma (x)$     : Sigmoid/逻辑(logistic)函数$\frac1{1+e^{-x}}$


#### 1.2.2 常见的二元函数

$a \land b$: 逻辑且(AND)

$a \lor b$: 逻辑或(OR)

$B(a,b)$: 贝塔函数

$ n\choose k$: n选k, 即$\frac{n!}{k!(n-k)!}$

$\delta_{ij}$: 克罗内罗克(Kronechker)$\delta$函数, 即$\delta_{ij}=\left\{\begin{array}{ll}1&(i=j)\\0&(i\neq j)\end{array}\right.$

$\boldsymbol{u}\circledast\boldsymbol{v}$: 两个向量的卷积(convolution)

#### 1.2.3 常见的多元函数

$B(\boldsymbol x)$: 多元贝塔函数$\frac{\prod_{k}\Gamma(x_{k})}{\Gamma(\sum_{k}x_{k})}$.

$\Gamma(\boldsymbol x)$: 多元伽马函数$\pi^{D(D-1)/4}\prod_{d=1}^{D}\Gamma\left(x+(1-d)/2\right)$

$\mathrm{softmax}(\boldsymbol x)$: Softmax函数$[\frac{e^{x_c}}{\sum_{c'=1}^Ce^{x_c'}}]_{c=1}^C$

## 1.3 线性代数(Linear algebra)

#### 1.3.1 向量(vector)

设$\boldsymbol{u}$, $\boldsymbol{v}$为N维向量, 则

$\boldsymbol{u}^{\mathsf{T}}\boldsymbol{v}$: 内积(inner/scalar product), ${\sum_{i=1}^{N}u_{i}v_{i}}$

$\boldsymbol{u}\boldsymbol{v}^{\mathsf{T}}$: 外积(outer product), $N\times N$矩阵

$\boldsymbol{u}\odot\boldsymbol{v}$: 哈达玛(Hadamard)积, $[u_{1}v_{1},\ldots,u_{N}v_{N}]$

$\boldsymbol{v}^T$: $\boldsymbol{v}$的转置(transpose)

$\dim(\boldsymbol{v})$: 向量$\boldsymbol{v}$的维度(dimensionality)

$\mathrm{diag}(\boldsymbol{v})$: 用向量$\boldsymbol{v}$生成一个$N\times N$的对角(diagonal)矩阵

$\boldsymbol{1}/\boldsymbol{1}_N$: 长度为N且全为1的向量

$\boldsymbol{0}/\boldsymbol{0}_N$: 长度为N且全为0的向量

$\|\boldsymbol{v}\| = \|\boldsymbol{v}\|_2$: 欧几里得距离(Euclidean)/$\ell_2$范数(norm), $\sqrt{\sum_{i=1}^Nv_i^2}$

$\|\boldsymbol{v}\|_1$: $\ell_1$范数, $\sum_{i=1}^N|v_i|$

#### 1.3.2 矩阵

设$\mathbf{S}$是一个$N\times N$的矩阵, $\mathbf{X}$和$\mathbf{Y}$为$M\times N$的矩阵, $\mathbf{Z}$为$M'\times N'$的矩阵, 则

$\mathbf{X}_{:,j}$: 矩阵的第j列(column)

$\mathbf{X}_{i,:}$: 矩阵的第i行(row)

$\mathbf{X}_{ij}$: 矩阵的第i行第j列元素

$\mathbf{S}\succ0$: $\mathbf{S}$是一个正定矩阵

$\mathrm{tr}(\mathbf{S})$: 矩阵的迹(trace)

$\det(\mathbf{S})$: 矩阵的行列式(determinant)

$|\mathbf{S}|$: 矩阵的行列式

$\mathbf{S}^{-1}$: 矩阵的逆(inverse)

$\mathbf{X}^{\dagger}$: 矩阵的伪逆(pseudo-inverse)

$\mathbf{X}^{T}$: 矩阵的转置

$\mathrm{diag}(S)$: 矩阵对角线上面的元素构成的向量

$\mathbf{I}/\mathbf{I}_N$: 单位矩阵(identity)

$\mathbf{X}\odot \mathbf{Y}$: 哈达玛积

$\mathbf{X}\otimes \mathbf{Z}$: 克罗内罗克积

#### 1.3.3 矩阵微积分(Matrix calculus)

**梯度(gradient)**

> 设$\boldsymbol{\theta}\in\mathbb{R}^N$为一个向量, $f:\mathbb{R}^N\to\mathbb{R}$为一个标量(scalar)函数, $f$的导数(derivative)可以写作
> $$
> \nabla_{\boldsymbol{\theta}}f(\boldsymbol{\theta})\triangleq\nabla f(\boldsymbol{\theta})\triangleq\nabla f\triangleq\begin{pmatrix}\frac{\partial f}{\partial\theta_1}&\cdots&\frac{\partial f}{\partial\theta_N}\end{pmatrix}
> $$
> 我们也可以写作
> $$
> \boldsymbol{g}_t\triangleq\boldsymbol{g}(\boldsymbol{\theta}_t)\triangleq\nabla f(\boldsymbol{\theta})\Big|_{\boldsymbol{\theta}_t}
> $$

**黑塞矩阵(Hessian)**

> 对上面的标量函数$f$求二阶导, 我们可以得到黑塞矩阵为
> $$
> \nabla^2f\triangleq\begin{pmatrix}\frac{\partial^2f}{\partial\theta_1^2}&\cdots&\frac{\partial^2f}{\partial\theta_1\partial\theta_N}\\&\vdots\\\frac{\partial^2f}{\partial\theta_N\theta_1}&\cdots&\frac{\partial^2f}{\partial\theta_N^2}\end{pmatrix}
> $$
> 我们也可以写作
> $$
> \mathbf{H}_t\triangleq\mathbf{H}(\boldsymbol{\theta}_t)\triangleq\nabla^2f(\boldsymbol{\theta})\bigg|_{\boldsymbol{\theta}_t}
> $$

## 1.5 优化(optimization)

$\boldsymbol \theta$: 需要被优化的变量

$\mathcal{L}(\boldsymbol \theta)$: 目标/成本函数

$\Theta$: 覆盖的$\boldsymbol \theta$集合

$\boldsymbol{\theta}_*\in\operatorname{argmin}_{\boldsymbol{\theta}\in\Theta}\mathcal{L}(\boldsymbol{\theta})$: $\boldsymbol \theta_*$是使得目标函数取得最小值的参数

**步长/学习率(step size/learning rate)**

> 在梯度下降(gradient descent)算法中我们有迭代(iteration)公式$\boldsymbol\theta_{t+1}=\boldsymbol\theta_t-\eta_t\boldsymbol{g}_t$, 这里的$\eta$为学习率.

## 1.6 概率论(Probability)

$p$: 概率密度函数(probability density function, pdf)/概率质量函数(probability mass function, pmf)

$P$: 累计分布函数(cumulative distribution function, cdf)

$\Pr$: 二元事件(binary event)的概率

$\hat p/q$: 表示近似(approximations to)服从分布$p$

$X$: 随机变量(random variable)

$x$: 随机变量的取值

$X\sim p$: 随机变量X服从p分布

$X\perp Y\mid Z$: 给定Z的条件下, X与Y是独立的, 即条件独立(conditionally independent)

**期望(expected value)和方差(variance)**

> 如果$X\sim p$, 我们记期望为
> $$
> \mathbb{E}\left[f(X)\right]=\mathbb{E}_{p(X)}\left[f(X)\right]=\mathbb{E}_X\left[f(X)\right]=\int_xf(x)p(x)dx
> $$
> 有时我们也记作$\overline{X}\triangleq\mathbb{E}\left[X\right]$, 我们记方差为
> $$
> \mathbb{V}\left[f(X)\right]=\mathbb{V}_{p(X)}\left[f(X)\right]=\mathbb{V}_X\left[f(X)\right]=\int_x(f(x)-\mathbb{E}\left[f(X)\right])^2p(x)dx
> $$

**协方差阵(covariance matrix)**

> 如果$\boldsymbol x$是一个随机向量, 我们记协方差阵为
> $$
> \mathrm{Cov}\left[\boldsymbol{x}\right]=\mathbb{E}\left[(\boldsymbol{x}-\overline{\boldsymbol{x}})(\boldsymbol{x}-\overline{\boldsymbol{x}})^\mathsf{T}\right]
> $$

**分布的模型(model)**

>如果$X\sim p$, 那么分布的模型记作
>$$
>\hat{x}=\operatorname*{mode}\left[p\right]=\operatorname*{argmax}_{x}p(x)
>$$

$p(\boldsymbol x|\boldsymbol \theta)$: 表示参数化(parametric)分布的pmf/pdf, $\boldsymbol x$为随机变量, $\boldsymbol \theta$分布的参数, 例如$\mathcal{N}(x|\mu,\sigma^2)$表示均值(mean)为$\mu$, 标准差(standard deviation)为$\sigma$的正态分布在x处的pdf.

## 1.7 信息论(Information theory)

**(微分)熵-(differential) entropy**

> 如果$X\sim p$, 我们记分布的(微分)熵为$\mathbb H(X)$或$\mathbb H(p)$. 

**相对熵/KL散度(relative entropy/Kullback-Leibler divergence)**

> 如果$X\sim p$, $Y\sim q$, 那么分布p相对于q的相对熵记作$D_{\mathbb{KL}}\left(p\parallel q\right)$.

$\mathbb{I}\left(X;{Y}\right)$: 随机变量的互信息(mutual information)

## 1.8 统计与机器学习(Statistics and machine learning)

#### 1.8.1 监督学习(Supervised learning)

$\boldsymbol{x}$: 特征/输入/协变量(features/inputs/covariates)

$\mathcal{X}$: $\boldsymbol{x}$的集合

$\boldsymbol{y}$: 目标/输出/响应变量(targets/outputs/response variables)

$\mathcal{Y}$: $\boldsymbol{y}$的集合

$\mathcal{D}$: 训练(training)集, $\mathcal{D}=\{(\boldsymbol{x}_n,\boldsymbol{y}_n):n\in\{1,\ldots,N\}\}$

$\mathbf{X}$: 训练输入矩阵

$\mathbf{Y}$: 训练输出矩阵

#### 1.8.2 无监督学习和生成模型(Unsupervised learning and generative)

$z_n$: 数据n的潜变量(latent variable)

$h_n$: 数据n的隐变量(hidden variable)

$v_n$: 数据n的可见变量(visible variable)

#### 1.8.3 贝叶斯推断(Bayesian inference)

在贝叶斯推断中，我们将参数的先验(prior)值写作$p(\boldsymbol{\theta}|\boldsymbol \xi)$，$\boldsymbol \xi$为超参数(hyperparameters)。对于共轭(conjugate)模型，后验(posterior)与先验是等价的。

$\stackrel \smile{\boldsymbol \xi}$: 通过超参数的先验分布值迭代更新

$\stackrel \frown{\boldsymbol \xi}$: 通过超参数的后验分布值迭代更新

$\boldsymbol\psi$: 变分(variational)后验的参数

$\boldsymbol \theta_s/\boldsymbol \theta^s$: 蒙特卡洛抽样(Monte Carlo sampling)中的样本(sample)

## 1.9 缩写(Abbreviations)

cdf: 累积分布函数

CNN: 卷积神经网络(Convolutional neural network)

DAG: 有向无环图(Directed acyclic learning)

DML: 深度度量学习(deep metric learning)

DNN: 深度神经网络

dof: 自由度

EB: 经验贝叶斯(empirical Bayes)

EM: 期望最大化算法(expectation maximization algorithm)

GLM: 广义线性模型(Generalized linear model)

GMM: 高斯混合模型(Gaussian mixture model)

HMC: 哈密顿蒙特卡洛(Hamiltonian Monte Carlo)

HMM: 隐马尔可夫模型(hidden Markov model)

idd: 独立同分布(independent and identically distributed)

iff: 当且仅当(if and only if)

KDE: 核密度估计(kernel density estimation)

KL: 相对熵/Kullback Leibler散度(divergence)

KNN: K近邻(nearest neighbor)

LHS: 方程左侧(left hand side)

LSTM: 长短期记忆(long short term memory)

LVM: 潜变量模型(latent variable)

MAP: 极大后验概率估计(maximum a posterior estimate)

MCMC: 马尔可夫链蒙特卡洛

MLE: 极大似然估计(maximum likelihood estimate)

MLP: 多层感知机(multilayer perceptron)

MSE: 均方误差(mean squared error)

NLL: 负对数似然(negative log likelihood)

OLS: 普通最小二乘法(ordinary least squares)

psd: 正定

pdf: 概率密度函数

pmf: 概率质量函数

PNLL: 受罚的负对数似然(penalized NLL)

PGM: 概率图模型(probabilistic graphical model)

RNN: 循环神经网络(recurrent neural network)

RHS: 方程右侧(right hand side)

RSS: 残差平方和(residual sum of squares)

rv: 随机变量

RVM: 相关向量机(relevance vector machine)

SGD: 随机梯度下降(stochastic gradient descent)

SSE: 误差平方和(sum of squared errors)

SVI: 随机微分推断(stochastic variational inference)

SVM: 支持向量机(support vector machine)

VB: 变分贝叶斯(variational Bayes)

wrt: 关于(with respect to)

----

**2024.5.26**

本节完结～～～
